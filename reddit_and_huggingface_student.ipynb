{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "# Reddit and HuggingFace Starter Kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: [Reddit API](https://www.reddit.com/dev/api/)\n",
    "The first part of this excercise is to figure out how to instantiate a Reddit API object using the Python Reddit API Wrapper [PRAW](https://praw.readthedocs.io/en/stable/).  PRAW is a Python library that provides a simple interfaceto interact with the Reddit API.\n",
    "\n",
    "### Your Task\n",
    "You will first need to instantiate a [Reddit instance](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html).\n",
    "Hint: you only need to use `client_id`, `client_secret`, and `user_agent`\n",
    "\n",
    "#### Make sure everyone in the group does this part! \n",
    "\n",
    "Follow the guide below on how to get your `client_id` and `client_secret`.\n",
    "\n",
    "#### Follow these steps:\n",
    "1. Pull the `FourthBrain/ML03` repo locally so you can start development.\n",
    "2. Open `reddit_and_huggingface.ipynb` and install the necessary packages for this lesson by running:\n",
    "\n",
    "    ```\n",
    "    cd code_student/Week_2\n",
    "    conda activate {your_virtual_environment_name}\n",
    "    pip install transformers praw torch torchvision torchaudio\n",
    "    ```\n",
    "    \n",
    "3. Obtain your `client_id` and `client_secret`\n",
    "\n",
    "* Make a Reddit account\n",
    "* Follow the steps in this screenshot which are the first steps from this [guide](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c).\n",
    "\n",
    "![instructions to set up reddit api](../../images/reddit_get_access.JPG)\n",
    "\n",
    "* Create a `secrets.py` file and include the following:\n",
    "\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"\"\n",
    "    REDDIT_API_USER_AGENT = {can_be_any_string...for ex: \"teslabot\"}\n",
    "    ```\n",
    "    Get it?  [Teslabot :)](https://www.tesla.com/AIhttps://www.tesla.com/AI)\n",
    "    \n",
    "\n",
    "* Put `secrets.py` in `Week_2` so you can easily import it\n",
    "\n",
    "4. Complete the code in the `# YOUR CODE HERE` space below that creates a reddit instance object that allows us to interact with the Reddit API.  Note that the `subreddit` object for the 'r/TSLA' subreddit has already been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from transformers import pipeline\n",
    "import secrets\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = secrets.REDDIT_API_CLIENT_ID,\n",
    "    client_secret = secrets.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent = secrets.REDDIT_API_USER_AGENT\n",
    ")\n",
    "\n",
    "subreddit = reddit.subreddit('TSLA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II:  [r/TSLA Subreddit](https://www.reddit.com/r/TSLA/)\n",
    "The second part of this exercise is to figure out how to the following code is parsing comments through use of the r/TSLA `subreddit` instance object.\n",
    "\n",
    "### Your Task\n",
    "1. Work with your group to comment each line of the following code so that you describe what each piece is doing.\n",
    "2. Create one comment at the top of the code that describes what the larger for loop is iterating over.  \n",
    "3. (Optional) How many comments will I get from this?\n",
    "\n",
    "A few resources that might help!\n",
    "* How do I find the top 10 posts of all time from your favorite subreddit(s)? (hint: look at [\"Obtain Submission Instances from a Subreddit\"](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html))\n",
    "* How do I parse comments from the post? (hint: look at [\"Obtain Submission Instances from a Subreddit\"](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of comments appearing in the top 10 submissions\n",
    "\n",
    "from praw.models import MoreComments                            #get MoreComments class\n",
    "\n",
    "top_comments = []\n",
    "\n",
    "for submission in subreddit.top(limit=10):                      #network request to iterate in r/TSLA.  NOTE: top returns best submissions of all time. comments may not be relevant for recent sentiment\n",
    "    for top_level_comment in submission.comments:               #iterate on all comments of a submission\n",
    "        if isinstance(top_level_comment, MoreComments):         #if the iteration gets to type MoreComments (Reddit API informs to continue to next page of comments) then skip to the next comment \n",
    "                    continue\n",
    "        top_comments.append(top_level_comment.body)             #if iteration is type comments then append the comments to the list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III:  [HuggingFace](https://huggingface.co/docs/transformers/quicktour)\n",
    "The third part of this exercise is to analyze the sentiment of each comment scraped from `r/TSLA` to using a pre-trained HuggingFace model to make the inference. \n",
    "\n",
    "### Your Task\n",
    "1. Implement the [Sentiment Analysis](https://huggingface.co/docs/transformers/quicktour) Model in the `# YOUR CODE HERE` section. \n",
    "2. (Optional) What is the net sentiment of the entire list of comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "# Build model and tokenizer from module pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")                #default model:  DistilBERT fine-tuned on the Stanford Sentiment Treebank v2 (SST2) task from the GLUE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb1a8333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment test: [deleted] === [{'label': 'NEGATIVE', 'score': 0.9992691874504089}]\n"
     ]
    }
   ],
   "source": [
    "# Run sentiment analysis on a random comment\n",
    "\n",
    "import random\n",
    "\n",
    "def get_random_comment(conversations):\n",
    "    comment = random.choice(conversations)\n",
    "    return comment\n",
    "\n",
    "sentiment_query_sentence = get_random_comment(top_comments) # grabs a random comment from the comment and replies list\n",
    "sentiment = sentiment_model(sentiment_query_sentence)       # get sentiment and score from the model\n",
    "print(f\"Sentiment test: {sentiment_query_sentence} === {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net sentiment is NEGATIVE.  score: 0.2689068888475795\n"
     ]
    }
   ],
   "source": [
    "# Optional: net sentiment of the entire list of comments\n",
    "\n",
    "ttl_score = 0                                               # 0 indicate neutral sentiment\n",
    "    \n",
    "for comment in top_comments:\n",
    "    sentiment = sentiment_model(comment)[0]\n",
    "    sentiment_label = sentiment['label']\n",
    "    sentiment_score = sentiment['score']\n",
    "    if sentiment_label == 'NEGATIVE':\n",
    "        ttl_score -= sentiment_score\n",
    "    else:\n",
    "        ttl_score += sentiment_score\n",
    "\n",
    "avg_score = abs(ttl_score/len(top_comments))             # score uniformly averaged on all comments\n",
    "\n",
    "if ttl_score == 0:\n",
    "    print('net sentiment is neutral')\n",
    "else:\n",
    "    if ttl_score > 0:\n",
    "        print('net sentiment is POSITIVE.  score:', avg_score)\n",
    "    else:\n",
    "        print('net sentiment is NEGATIVE.  score:', avg_score)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
